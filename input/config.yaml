EXPERIMENT:
  name: "w_emb_wo_label_w_pred"
  n_pred_vectors: 1
  n_noise_samples: 0
  k_fold: 1


DATASET:
  names: ["bank_marketing", "gesture_phase", "adult", "heloc"]
  one_hot: false
  ratio: 0.1
  force: true
  baseline: "xgboost" # neural_network / xgboost
  pd_dataframe: true


CLOUD:
  name: "patch" # tabular / ensemble / dense / efficientnet / resnet_embedding / casual_llm / masked_llm / patch
  epochs: 10
  top_k: 100000
  logits: false # True - Use LLM logits, False - Use llm's vocab probability
  models: ['RandomForestClassifier', 'BernoulliNB', 'CalibratedClassifierCV', 'LinearDiscriminantAnalysis', 'LogisticRegression', 'AdaBoostClassifier', 'ExtraTreesClassifier', 'XGBClassifier', 'LGBMClassifier']

IIM:
  name: "xgboost" # xgboost / dense
  epochs: 10
  batch_size: 8
  dropout_rate: 0.3

ENCRYPTOR:
  name: "efficientnet" # dc / dense_complex / dense / resnet / efficientnet
